---
title: "R Crawler2 POST 함수로 수집하기"
author: "Dr.Kevin"
date: "1/19/2018"
output: 
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, out.width = '750px', dpi = 300)
```

## POST 함수로 웹데이터 수집하기

인터넷에서 페이지를 이동할 때마다 URL이 바뀌는 경험을 하셨을 것입니다. 하지만 그렇지 않은 웹페이지도 많습니다. URL은 uniform resource locator의 머릿글자로 웹페이지의 주소라고 할 수 있습니다. 그렇기에 "GET 함수로 수집하기"에서는 **HTTP Request**를 할 때 URL만으로도 가능했습니다.

이번에는 화면이 바뀌어도 URL이 고정되어 있는 사이트의 정보를 수집할 때 **POST** 함수를 이용하는 방법을 알아보도록 하겠습니다. 패키지는 지난 번과 동일하게 불러오기 하면 됩니다.

```{r message=FALSE}
# 패키지 불러오기
library(httr)
library(rvest)
library(dplyr)
library(magrittr)
```

이번에 한국 전화번호부(http://www.isuperpage.co.kr) 웹페이지에서 추천맛집 주소와 전화번호를 수집하는 예제를 실행해보도록 하겠습니다. 웹페이지에 들어가서 원하는 조건을 선택해봐도 URL이 바뀌지 않는 것을 알 수 있습니다. 이럴 때 GET 함수로 웹페이지를 가져와도 메인 페이지의 html만 수집이 될 것입니다.

메인페이지 중앙에 있는 업종 메뉴에서 "음식/주점 > 추천맛집"을 선택한 후, 우측 상단에 있는 지역 선택 메뉴에서 "여의도동"을 입력하면 "서울 영등포구 여의도동"을 선택할 수 있습니다. 이 상태에서 10개의 추천맛집 리스트가 노출되는 것을 확인할 수 있습니다.

화면 아무 곳에서나 오른쪽 클릭을 한 후 "검사(Inspect)"를 선택하면 개발자도구가 열립니다. 개발자도구 맨 상단 메뉴 중 **Network**로 이동합니다.  

![](https://github.com/MrKevinNa/MrKevinNa.github.io/blob/master/images/Chrome%20Developer%20Tools%20Image.png?raw=true)

그 상태에서 개발자도구 왼쪽 상단에서 **clear**와 **recording network log**를 클릭합니다. clear는 빈 원에 직선이 사선으로 그어저 있는 모양이고, recording network log는 회색의 원 모양입니다. recording 중일 때 빨간색으로 바뀝니다.

이제 새로고침(F5) 합니다. 그러면 뭔가 바쁘게 움직이는 걸 볼 수 있습니다. 하단에 새로 생긴 목록에서 맨 처음 나오는 **search.asp**에 주목합니다.  

![](https://github.com/MrKevinNa/MrKevinNa.github.io/blob/master/images/Chrome%20Developer%20Tools%20Network%200.png?raw=true)

**search.asp**을 클릭하면 우측에 새로운 화면이 열리는데 요청과 응답에 필요한 정보들을 확인할 수 있습니다.  
  - Request URL은 **POST** 함수에 할당할 URL입니다.  
  - Request Method는 "POST"로 되어 있으니 **POST** 방식으로 요청을 해야 합니다.  
  - Status Code는 "200"으로 정상입니다.  

![](https://github.com/MrKevinNa/MrKevinNa.github.io/blob/master/images/Chrome%20Developer%20Tools%20Network%201.png?raw=true)

맨 아래로 이동하여 **Form Data**를 확인합니다. POST 함수로 웹페이지 정보를 수집할 때 Form Data를 리스트의 인자로 할당하기 때문에 이 정보들이 필요합니다.  

![](https://github.com/MrKevinNa/MrKevinNa.github.io/blob/master/images/Chrome%20Developer%20Tools%20Network%202.png?raw=true)

그런데 한 가지 복병이 생겼습니다! **Form Data**에 일부 항목값이 `(unable to decode value)`로 되어 있습니다. 기계가 문자를 인식하게 하기 위해 **부호화(Percent-Encoding)** 해준 것을 사람이 이해할 수 있도록 **복호화(Percent-Decoding)** 해주어야 하는데, 현재 상태로는 그렇게 되지 않는 것으로 보입니다. **Form Data** 오른쪽 끝에 `view URL encoded`를 클릭하면 아래 그림처럼 알기 어려운 문자들이 보입니다. 즉, 기계가 문자를 인식할 수 있는 **(%-인코딩 된)** 상태인 것입니다.  

![](https://github.com/MrKevinNa/MrKevinNa.github.io/blob/master/images/Chrome%20Developer%20Tools%20Network%203.png?raw=true)

여기서 한 가지 문제가 더 있는데요. (%-인코딩은 일단 잊어버리고) 각 언어마다 기계가 인식할 수 있도록 인코딩하는 방식이 제각각이었고, 이를 통일하기 위해 국제표준이라 할 수 있는 **UTF-8**이 개발되었습니다. 웹페이지를 개발할 때 인코딩 방식으로 **UTF-8**을 쓰면 좋은데, 한국은 윈도우즈가 많이 사용되어서 그런지 윈도우즈에서 개발한 **EUC-KR** 또는 **CP949**를 사용하는 경우가 많습니다.  

![한글 인코딩](https://byjason.github.io/images/posts/character_encoding.png)  
- [참조] https://byjason.github.io/2016-08-30/web-scraping-with-r(1)/

위 그림에서 보면 **EUC-KR**은 **CP949**에 포함되는 부분집합임을 알 수 있으며, **UTF-8**과 교집합으로 **ASCII**가 있습니다. **ASCII**는 미국에서 알파벳과 숫자 및 주요 기호들을 1 byte로 표현한 것입니다. 따라서 어떤 인코딩 방식을 사용해도 영문과 숫자, 기호들은 멀쩡하게 보이는 것입니다.

이번 예제에서 우리가 수집하려고 하는 웹페이지도 **EUC-KR** 방식을 사용하고 있습니다. 우리는 RStudio의 Global Options에서 기본 인코딩을 **UTF-8**으로 설정한 바 있습니다. 현재 R 환경에서 문자열을 인식하는 인코딩 방식을 확인해보겠습니다.

```{r}
# 인코딩 방식 확인
localeToCharset()
```

결과가 **UTF-8**로 나왔습니다.

그럼 특정 문자열의 인코딩 방식을 확인해보겠습니다. `Encoding("문자열")` 함수를 이용합니다.

```{r}
# 문자열의 인코딩 확인
Encoding("추천맛집")
```

다행하게도 **unknown**으로 확인되었습니다. 하지만 앞에서 **localeToCharset** 함수로 확인한 값이 "UTF-8"이었으므로 이 문자열의 인코딩도 "UTF-8"으로 인식될 것입니다.

문제는 웹페이지가 **EUC-KR** 방식을 사용하고 있으므로 POST 함수의 인자로 한글을 입력할 때 **EUC-KR** 또는 **CP949**로 되어 있어야 한다는 것입니다. 특정 문자열의 인코딩 변경은 `iconv` 함수를 사용합니다.

```{r}
# 문자열의 인코딩 방식 변환
iconv(x = "추천맛집", from = "UTF-8", to = "CP949")
```

인코딩을 변경하니 괴상한 문자들이 출력되었습니다. 아무튼 이 웹페이지에 한글을 인식시키려면 인코딩을 **CP949**로 바꿔준 다음 **POST** 함수의 body 인자에 할당해주면 됩니다.

이제 드디어 **POST** 함수를 등장시켜 보겠습니다. 먼저 URL과 위에서 확인한 **Form Data**의 값들을 다음과 같이 설정해보겠습니다.

```{r}
# 웹사이트 url 지정
url <- "http://www.isuperpage.co.kr/search.asp"

# POST 인자 지정
keyword <- iconv(x = "추천맛집", from = "UTF-8", to = "CP949")
sido <- iconv(x = "서울", from = "UTF-8", to = "CP949")
gugn <- iconv(x = "영등포구", from = "UTF-8", to = "CP949")
dong <- iconv(x = "여의도동", from = "UTF-8", to = "CP949")
```

위에서 설정한 값들을 이용하여 웹페이지를 가져오겠습니다. **POST** 함수의 인자 중 body는 리스트 형태를 가져야 하며, 리스트의 인자명은 **Form Data**에서 사용된 것과 동일하게 설정해주어야 합니다. 이 때, 각각의 인자에 **I** 함수를 적용해주어야 제대로 작동합니다.

**I** 함수는 이중 인코딩(double encoding)을 막기 위해 사용하는 것인데요. `httr` 패키지는 문자열에 대해 **%-인코딩**(또는 **escape**이라고도 함) 처리를 자동으로 해줍니다. 문제는 타겟 웹사이트가 **EUC-KR** 인코딩 방식을 사용하는 경우, `POST()`의 인자값으로 문자열을 할당하면 제대로 인식하지 못하고 결국 사용자가 원하는 웹데이터를 응답받지 못합니다. 반면에, 웹페이지가 **UTF-8** 방식을 사용하고 있으면 **I** 함수를 사용하지 않아도 된다고 합니다. (도움을 주신 분들께 감사드립니다. ^^)

```{r}
# html request
resp <- POST(url = url, 
             encode = "form",
             body = list(searchWord = I(keyword),
                         city = I(sido),
                         gu = I(gugn),
                         dong = I(dong)
             )
)
```

resp의 응답 상태코드를 확인해보겠습니다. 200이면 정상입니다.

```{r}
# 응답 상태코드 확인
status_code(resp)
```

이제 html로부터 원하는 데이터를 정리하는 작업이 남았습니다. 웹브라우저에서 상호명에 마우스를 가져다 놓고 오른쪽 클릭 후 **검사(Inspect)**를 선택하여 식당 정보가 반복되는 구조를 찾습니다.

`<dif id="search_resutl>`의 하위 tag로 `<div class="tit_list">`와 `<div class="l_cont">`를 정리하면 원하는 데이터를 얻을 수 있을 것 같습니다. 먼저 상호명만 따로 추출하여 10개의 길이를 가진 문자 벡터로 만들어 보겠습니다.

```{r}
# 상호명 추출하기
shopn <- resp %>% 
  read_html(encoding = "EUC-KR") %>% 
  html_nodes(css = "div#search_result ol li div.tit_list") %>% 
  html_text()

print(shopn)
```

모든 상호명 뒤에 공통적으로 "추천맛집"이라는 문자열이 추가되어 있습니다. **stringr** 패키지의 **str_replace** 함수를 사용하여 삭제하도록 하겠습니다.

```{r}
# 필요한 패키지 불러오기
library(stringr)

# 상호명 추출하고 불필요한 문자열 삭제하기
shopn <- resp %>% 
  read_html(encoding = "EUC-KR") %>% 
  html_nodes(css = "div#search_result ol li div.tit_list") %>% 
  html_text() %>% 
  str_replace(pattern = "추천맛집", replacement = "")

print(shopn)
```

다음으로 전화번호, 지번주소, 도로명주소를 수집해보겠습니다. 최종 텍스트 앞뒤로 공백이 생길 수 있으므로 이번에는 **stringr** 패키지의 **str_trim** 함수를 사용하도록 하겠습니다.

```{r}
# 기타 정보 추출하기
sInfo <- resp %>% 
  read_html(encoding = "EUC-KR") %>% 
  html_nodes(css = "div#search_result ol li div.l_cont span") %>% 
  html_text() %>% 
  str_trim()

print(sInfo)
```

sInfo는 10개 상호에 해당하는 정보가 반복되고 있음을 알 수 있습니다. 따라서 데이터의 위치에 따라 다음과 같이 추출하도록 합니다.  

  - 전화번호 : 1, 5, 9와 같이 4로 나누어 나머지가 1인 위치에 있는 데이터  
  - 지번주소 : 2, 6, 10과 같이 4로 나누어 나머지가 2인 위치에 있는 데이터  
  - 도로명주소 : 4, 8, 12와 같이 4로 나누어 나머지가 0인 위치에 있는 데이터  

```{r}
# 기타 정보 벡터로 정리
phone <- sInfo[(0:9) * 4 + 1]
jibun <- sInfo[(0:9) * 4 + 2]
roadn <- sInfo[(0:9) * 4 + 4]
```

마지막으로 지금까지 수집한 벡터들로 데이터 프레임을 만듭니다.

```{r}
# 최종 데이터 프레임 생성
result <- data.frame(shopn = shopn,
                     phone = phone,
                     jibun = jibun,
                     roadn = roadn)

print(result)
```

이상으로 **POST** 함수를 이용하여 웹데이터를 수집하는 방법을 알아보았습니다. 다음에는 User-agent를 추가하여 웹크롤러가 아닌 사람이 요청(Request)하는 것처럼 보이는 방법에 대해서 간단하게 알아보겠습니다.

## [추가] 페이지 네이게이션 활용하기

이번 예제에서는 조회 조건에 따라 1페이지에 출력된 데이터만 수집하는 것을 보여드렸는데요. 2페이지 이상 출력되는 경우, 모든 데이터를 수집하려면 어떻게 해야 하는지 궁금하실 것 같아서 추가하였습니다.

타겟 웹사이트에서 지역과 업종을 선택하면 1페이지에 최대 10개의 식당 리스트가 출력됩니다. 화면을 맨 밑으로 내리면 숫자가 적힌 버튼이 보이고, 이 숫자 버튼을 클릭하면 해당 페이지로 이동하는데요. 2페이지부터는 아래 방식으로 웹데이터를 수집할 수 있습니다.

먼저 크롬 개발자도구를 열고 **네트워크** 탭으로 이동합니다. **clear** 버튼을 눌러 아래 내용을 깨끗하게 지운 후, 네비게이션 위치로 가서 **2**를 클릭합니다. 그러면 여러 항목이 뜨는데 모두 GET 방식을 사용하고 있습니다. 좀 놀랍네요.

맨 위에 있는 항목을 유심히 살펴보니, 위에서 `POST()` 방식으로 html 요청할 때 **Form Data**의 요소로 쓰인 것들이 일부 보이네요. 이걸 클릭하고 들어가서 화면을 맨 아래로 내려보니 **Form Data** 대신 **Query String Parameters**가 보이고 세부 요소들이 **Form Data**의 요소들과 많이 일치하는 것을 알 수 있습니다. 그리고 **page**라는 요소가 추가로 보입니다.

일단 **GET** 방식이 사용된다는 것에 주목해봅시다. 화면을 다시 맨 위로 올려서 **General**의 **Request URL**을 살펴보면, 위에서 `POST()` 함수의 인자로 사용된 요소들이 하나의 URL로 조립되어 있다는 것을 알 수 있습니다. 그렇다면 **Request URL**을 분해한 다음, 각 요소를 다시 조립하는 방식으로 타겟 URL을 만들어 낼 수 있을 것입니다!  

### 타겟 URL 분해하고 조립하기

크롬 개발자도구에서 **Request URL**을 복사하여 붙인 후, 구분자(?, &)를 기준으로 분해하면 다음과 같습니다.  

  - `http://isuperpage.co.kr/search/s_pagedata_page.asp` : 공통부분  
  - `?x=37.5290841` : 사용자의 경도 좌표(로 추정)  
  - `&y=126.9293792` : 사용자의 위도 좌표(로 추정)  
  - `&searchWord=%C3%DF%C3%B5%B8%C0%C1%FD` : 검색어 (여기서는 "추천맛집")  
  - `&page=2` : 페이지 번호  
  - `&city=%BC%AD%BF%EF` : 광역시도 (여기서는 "서울")  
  - `&gu=%BF%B5%B5%EE%C6%F7%B1%B8` : 시구군 (여기서는 "영등포구")  
  - `&dong=%BF%A9%C0%C7%B5%B5%B5%BF` : 읍면동 (여기서는 "여의도동")  
  - `&addr4=` : 완성된 주소(로 보이는데 할당된 값이 없으므로 무시)  
  - `&pdc=0` : 모르겠음(역시 무시함)  

`searchWord` 요소에 할당된 `%C3%DF%C3%B5%B8%C0%C1%FD`을 사람이 읽을 수 있도록 변환해보겠습니다. 먼저 적당한 객체명에 할당해줍니다. 그리고 R base 함수인 `URLdecode()` 함수를 사용해보겠습니다.

```{r}
# Request URL에서 searchWord에 할당된 값 저장
encoded <- "%C3%DF%C3%B5%B8%C0%C1%FD"

# %-인코딩된 문자열을 사람이 이해할 수 있도록 디코딩하기
URLdecode(encoded)
```

위와 같이 디코딩을 해보니 Windows 사용자들은 `추천맛집`이라고 잘 보일 것입니다. 하지만 Mac 사용자들은 `\xc3\xdfõ\xb8\xc0\xc1\xfd`으로 출력되어 마치 외계어처럼 보일 것입니다. 그 이유를 짐작하실 수 있나요? 바로 R이 인코딩하는 방식에 있습니다. Windows 사용자들이 RStudio에서 `localeToCharset()`을 실행하면 **CP949**로 출력될 것입니다. 이번 예제의 타겟 웹사이트가 인코딩 방식으로 **EUC-KR**을 사용하고 있고, **EUC-KR**은 **CP949**의 부분집합임을 고려할 때 서로 인코딩 방식이 맞는 거죠. 그런데 Mac 사용자라면 `localeToCharset()` 결과로 **UTF-8**이 출력될 것이고, 이것은 **EUC-KR**과 다르기 때문에 인코딩 방식이 충돌하는 거죠. 그래서 한글이 이상하게 외계어처럼 보이는 것입니다.

이 문제를 해결하려면? 아래와 같이 `iconv()`를 사용하면 됩니다. Mac 사용자만 해보세요. 

```{r}
# 인코딩 변환 (Mac 사용자만 실행해보세요)
iconv(x = URLdecode(encoded), from = "CP949", to = "UTF-8")
```

이렇게 하면 Mac 사용자도 `추천맛집`이라고 잘 보일 것입니다. **%-디코딩**하는 방법을 알았으니 **%-인코딩**하는 방법을 알아보겠습니다. 이 방법을 알아야 `GET()`의 `url` 인자에 할당할 `타겟 url`을 제대로 조립할 수 있습니다.

**%-인코딩**은 R base 함수인 `URLencode()` 함수를 사용하면 해결됩니다.

```{r}
# 한글 문자열 지정
decoded <- "추천맛집"

# %-인코딩 하기
URLencode(decoded)
```

출력된 결과를 살펴보면, Windows 사용자는 `encoded`와 똑같다는 것을 확인할 수 있을 것입니다. 하지만 Mac 사용자는 `%EC%B6%94%EC%B2%9C%EB%A7%9B%EC%A7%91`로 출력되어 `encoded`와 많이 다를 것입니다.

참고로 두 문자열이 정확하게 일치하는 지 확인하려면 비교연산자 `==`를 사용하면 됩니다. 만약 두 개의 문자열이 똑같다면 `TRUE`를 출력할 것입니다.

```{r}
# 두 문자열 일치 여부 확인
# Windows 사용자는 TRUR, Mac 사용자는 FAlSE가 출력됨
encoded == URLencode(decoded)
```

Mac 사용자의 경우, 역시 인코딩 문제 때문에 이런 현상이 발생합니다. 만약 `iconv()`를 이용해서 문자열의 인코딩 방식을 **UTF-8**에서 **CP949**로 바꾸어주면 어떨까요? 한 번 해보겠습니다.

```{r}
# 문자열의 인코딩 방식 확인
Encoding(decoded)

# 문자역의 인코딩 방식 변경
iconv(x = decoded, from = "UTF-8", to = "CP949")

# 다시 %-인코딩 하기
URLencode(iconv(x = decoded, from = "UTF-8", to = "CP949"))
```

결과는 로케일이 맞지 않다는 이유와 함께 아쉽게도 `NA`를 출력하였습니다. 그래서 로케일을 변경한 후에 다시 `%-인코딩`을 해보겠습니다. Windows 사용자는 실행할 필요 없습니다!

```{r}
# Mac에서 로케일 초기화
Sys.setlocale(category = "LC_ALL")

# 로케일 확인
Sys.getlocale()

# UTF-8에서 CP949로 로케일 변경
Sys.setlocale(category = "LC_ALL", locale = "ko_KR.CP949")

# 다시 %-인코딩 하기
URLencode(iconv(x = decoded, from = "UTF-8", to = "CP949"))

# 두 문자열 일치 여부 확인
encoded == URLencode(iconv(x = decoded, from = "UTF-8", to = "CP949"))
```

이제야 비로소 두 문자열이 일치합니다. 로케일에 익숙하지 않은 분들은 이 방법 말고 좀 더 쉬운 방법을 소개해드리겠습니다. 바로 `urltools` 패키지의 `url_encode()` 함수를 사용하는 것입니다. 역시 Windows 사용자는 실행하지 않습니다.

```{r}
# Mac에서 로케일 초기화
Sys.setlocale(category = "LC_ALL")

# 필요한 패키지 불러오기
library(urltools)

# %-인코딩 하기
url_encode(iconv(x = decoded, from = "UTF-8", to = "CP949"))
```

출력된 결과가 `encoded`와 비슷해 보이지만 대문자가 아닌 소문자로 출력되었습니다. 두 문자열이 똑같은 지 확인하기 위해 방금 실행한 스크립트를 `toupper()` 함수에 할당하여 모두 대문자로 변환한 후 일치 여부를 확인해보겠습니다. 

```{r}
# 대문자로 변환
toupper(url_encode(iconv(x = decoded, from = "UTF-8", to = "CP949")))

# 일치 여부 확인
encoded == toupper(url_encode(iconv(x = decoded, from = "UTF-8", to = "CP949")))
```

먼 길을 돌아왔지만 결국 해결했습니다. 참고로 url은 대소문자 구분을 하지 않기 때문에 타겟 URL이 소문자로 되어 있어도 제대로 실행은 될 것입니다. 따라서 `url_encode()` 함수를 이용하여 한글 문자열을 `%-인코딩` 처리한 후 타겟 URL을 조립하면 됩니다.

타겟 URL을 조립할 때 필요한 요소만 사용하겠습니다. 그러니까 `x`, `y`, `addr4` 및 `pdc`는 제외하고 `searchWord`, `city`, `gu`, `dong` 및 `page`만 추가하겠습니다.

```{r}
# 조회 기준 설정
upjong <- "한식"
sido <- "서울"
gugn <- "강남구"
dong <- "압구정동"

# 타겟 URL 요소 설정 (Windows 사용자만 실행하세요!)
url <- paste0(
  "http://isuperpage.co.kr/search/s_pagedata_page.asp",
  paste0("?searchWord=", URLencode(upjong)),
  paste0("&city=", URLencode(sido)),
  paste0("&gu=", URLencode(gugn)),
  paste0("&dong=", URLencode(dong)),
  paste0("&page=", 1)
  )

# Mac용 %-인코딩 함수 생성
pcntEncoding4Mac <- function(string) {
  encoded <- toupper(url_encode(iconv(x = string, from = "UTF-8", to = "CP949")))
  return(encoded)
}

# 타겟 URL 요소 설정 (Mac 사용자만 실행하세요!)
url <- paste0(
  "http://isuperpage.co.kr/search/s_pagedata_page.asp",
  paste0("?searchWord=", pcntEncoding4Mac(upjong)),
  paste0("&city=", pcntEncoding4Mac(sido)),
  paste0("&gu=", pcntEncoding4Mac(gugn)),
  paste0("&dong=", pcntEncoding4Mac(dong)),
  paste0("&page=", 1)
  )

# 타겟 URL 출력
cat(url)
```
타겟 URL을 복사하여 웹브라우저에서 접속해보겠습니다. 화면에 텍스트만 출력되네요. 이런 형태를 **JSON**이라고 합니다. 길고 긴 이번 포스팅의 마지막 부분은 R에서 **JSON**을 처리하는 방법을 안내하겠습니다.

### JSON 데이터 처리하기

데이터를 주고 받는 목적으로 사용되는 형태로 **XML**과 **JSON**을 들 수 있는데요. **XML**은 **[R에서 Open API 활용하기](https://mrkevinna.github.io/R%EC%97%90%EC%84%9C-Open-API-%ED%99%9C%EC%9A%A9%ED%95%98%EA%B8%B0/)**에서 소개해드릴 예정입니다.

**JSON**은 **JavaScript** 방식으로 처리된 데이터 교환 형태라고 생각하면 쉽습니다. **XML**에 비해 가볍고, 데이터 처리가 좀 더 쉽다는 장점이 있습니다.

이번 예제에서 필요한 함수는 `jsonlite` 패키지의 `fromJSON()`입니다. 먼저 타겟 URL로 html 요청한 후 **JSON** 데이터를 다뤄보겠습니다.

```{r}
# GET()로 html 요청하기
resp <- GET(url)

# resp 출력
cat(content(x = resp, as = 'text'))
```

```{r}
# 필요한 패키지 불러오기
library(jsonlite)

# JSON 데이터 처리하기
fromJSON(txt = as.character(resp))
```

`fromJSON()` 함수를 실행한 결과, 4개의 요소를 갖는 리스트가 출력되었습니다. 각 요소가 의미하는 것은 다음과 같습니다.  

  - `totalCount` : 조회 조건에 맞는 식당은 총 7개  
  - `pageSize` : 한 페이지당 출력하는 식당의 개수  
  - `currentPage` : 현재 응답받은 페이지 위치  
  - `poiList` : 현재 응답받은 식당 정보를 데이터 프레임으로 생성  

첫 번째 요소인 `totalCount`를 두 번째 요소인 `pageSize`로 나누면 순환 실행할 전체 페이지 수를 계산해낼 수 있습니다. 그리고 각 페이지에서 출력되는 식당 정보는 네 번째 요소로 깔끔하게 정리되어 있으므로 네 번째 요소만 가져와서 `rbind()`로 처리하면 간단하게 해결됩니다.

```{r}
# 최종 데이터 객체
result <- fromJSON(txt = as.character(resp))[[4]]
```

이상으로 길고 긴 이번 포스트를 마무리합니다.
